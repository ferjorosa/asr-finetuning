# Precision
precision: "bf16-mixed"

# Optimizer and training stability
weight_decay: 0.001

# Learning-rate schedule
learning_rate: 1.0e-4
warmup_steps: 5

# Tokens, steps, and batching
# effective batch size = batch_size * gradient_accumulation_steps * num_devices
batch_size: 96
gradient_accumulation_steps: 4
num_epochs: 1

# Validation and logging
val_every_n_steps: 5
system_metrics_every_n_steps: 10

# Checkpointing
save_every_n_steps: 500
output_dir: "outputs"
resume_from_checkpoint: null
