# Precision
precision: "bf16-mixed"

# Optimizer
weight_decay: 0.01
betas: [0.9, 0.95]
eps: 1.0e-8
grad_clip_norm: 1.0

# Learning-rate schedule
learning_rate: 1.0e-4
warmup_steps: 50
scheduler: "cosine"
min_lr: 1.0e-5

# Tokens, steps, and batching
# effective batch size = batch_size * gradient_accumulation_steps * num_devices
batch_size: 96
gradient_accumulation_steps: 4
num_epochs: 1

# Validation and logging
val_every_n_steps: 5
system_metrics_every_n_steps: 10

# Checkpointing
save_every_n_steps: 500
output_dir: "outputs"
resume_from_checkpoint: null
