# Precision
precision: "bf16-true"

# Optimizer
optimizer: "adamw_8bit"
weight_decay: 0.01
betas: [0.9, 0.95]
eps: 1.0e-8
grad_clip_norm: 1.0

# Learning-rate schedule
learning_rate: 1.0e-4
warmup_steps: 50
scheduler: "cosine"
min_lr: 1.0e-5

# effective batch size = batch_size * gradient_accumulation_steps * num_devices
batch_size: 96
gradient_accumulation_steps: 4
num_epochs: 1

# Validation and logging
#
# The dataset has 1195 rows
# - 1123 training
# - 72 validation
# Each batch will consider 96 rows so with grad_accumulation of 4,
# we would need to validate every step to show something
val_every_n_steps: 1
limit_val_batches: 0.0 # Set to 0 to disable validation
system_metrics_every_n_steps: 1

# Checkpointing
save_every_n_steps: 500 # not gonna be used since the dataset is tiny + LoRA
output_base_dir: "runs"
resume_from_checkpoint: null
